{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to install matplotlib, seaborn, pandas and mpld3 to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for macOS\n",
    "# !pip3 install --user matplotlib seaborn pandas mpld3\n",
    "!pip install matplotlib seaborn pandas mpld3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from plot_fitness import *\n",
    "import mpld3\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leonhard_directory = \"test_scaling_of_naive_Nov_14_115812\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments, dataframes = extract_all_run_values(leonhard_directory)\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(ax=ax, x=\"epoch\", y=\"fitness\", data=dataframes[0][dataframes[0].epoch > 150])\n",
    "ax.set_title(experiments[0])\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"naive_n_32.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(ax=ax, x=\"epoch\", y=\"fitness\", data=dataframes[4][dataframes[4].epoch > 150])\n",
    "ax.set_title(experiments[4])\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"naive_n_1.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory name is needed\n",
    "\n",
    "data_dir = \"grid_search_island_Nov_15_163809\"\n",
    "\n",
    "\n",
    "# parse JSON to identify variable parameters\n",
    "\n",
    "all_names = os.listdir(data_dir)\n",
    "\n",
    "json_filename = list(filter(lambda x: \".json\" in x, all_names))\n",
    "\n",
    "if len(json_filename) == 0:\n",
    "    print(\"No JSON file specifying the experiment was found in directory {}.\".format(directory_name))\n",
    "    exit(1)\n",
    "if len(json_filename) > 1:\n",
    "    print(\"Found multiple JSON files ({}) in directory {}.\".format(json_filename, directory_name))\n",
    "\n",
    "json_filename = json_filename[0]\n",
    "\n",
    "\n",
    "try:\n",
    "    json_file = open(os.path.join(data_dir, json_filename), mode=\"r\")    \n",
    "except OSError:\n",
    "    print(\"Failed to open file {}.\".format(json_filename))\n",
    "    exit(1)\n",
    "    \n",
    "json_experiment = json.load(json_file)\n",
    "json_file.close()\n",
    "\n",
    "\n",
    "# print the contents of the JSON file\n",
    "\n",
    "json_experiment_name = json_experiment[\"name\"] # str\n",
    "assert(type(json_experiment_name) == str)\n",
    "\n",
    "print(\"<name>\")\n",
    "print(json_experiment_name)\n",
    "\n",
    "json_repetitions = json_experiment[\"repetitions\"] # int\n",
    "assert(type(json_repetitions) == int)\n",
    "\n",
    "print(\"<repetitions>\")\n",
    "print(json_repetitions)\n",
    "\n",
    "json_fixed_params = json_experiment[\"fixed_params\"] # dictionary\n",
    "assert(type(json_fixed_params) == dict)\n",
    "\n",
    "print(\"<fixed parameters>\")\n",
    "for key, value in json_fixed_params.items():\n",
    "    print(\"{} {}\".format(key, value))\n",
    "\n",
    "json_variable_params = json_experiment[\"variable_params\"] # dictionary\n",
    "assert(type(json_variable_params) == dict)\n",
    "\n",
    "print(\"<variable parameters>\")\n",
    "for key, value in json_variable_params.items():\n",
    "    \n",
    "    # value is a dictionary with\n",
    "    assert(type(value[\"type\"]) == str)\n",
    "    assert(type(value[\"list\"]) == list)\n",
    "    \n",
    "    value_type = value[\"type\"]\n",
    "    assert(value_type == \"list\")\n",
    "    \n",
    "    value_range = value[\"list\"]\n",
    "    \n",
    "    print(\"{} {}\".format(key, value_range));\n",
    "\n",
    "\n",
    "# store the variable parameters\n",
    "    \n",
    "variable_params_names = list()\n",
    "variable_params_lists = list()\n",
    "\n",
    "for key, value in json_variable_params.items():\n",
    "    \n",
    "    variable_params_names.append(key)\n",
    "    variable_params_lists.append(value[\"list\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the idea is to get the value of all variable parameters from the file 'leonhard.log'\n",
    "# which is stored inside each folder of the experiment (each folder contains a separate job)\n",
    "\n",
    "logfile_name = \"leonhard.log\"\n",
    "\n",
    "# list of all job directories\n",
    "\n",
    "job_dirs = [os.path.join(data_dir, f) for f in os.listdir(data_dir) \\\n",
    "            if os.path.isdir(os.path.join(data_dir, f))]\n",
    "\n",
    "\n",
    "for job_dir in job_dirs:\n",
    "    \n",
    "    print(\"<{}>\".format(job_dir))\n",
    "    \n",
    "    logfile_path = os.path.join(job_dir, logfile_name)\n",
    "    \n",
    "    try:\n",
    "        logfile = open(logfile_path, mode=\"r\")\n",
    "    except OSError:\n",
    "        print(\"Filed to open {}.\".format(logfile_path))\n",
    "        exit(1)\n",
    "    \n",
    "    \n",
    "    for param_name in variable_params_names:\n",
    "        \n",
    "        param_found = False\n",
    "        \n",
    "        for line in logfile:\n",
    "            for word in line.split():\n",
    "                \n",
    "                if(param_found):\n",
    "                    print(\"{} {}\".format(param_name, word))\n",
    "                    break\n",
    "                    \n",
    "                if(param_name == word):\n",
    "                    param_found = True\n",
    "                    \n",
    "            if(param_found):\n",
    "                break\n",
    "                \n",
    "        # reset the cursor to the beginning of the file\n",
    "        logfile.seek(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for rep in range(json_repetitions):\n",
    "    print(rep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the big pandas tables\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../logging')\n",
    "from process_log import Tags, Log, Epochs\n",
    "\n",
    "tags = Tags(\"../logging/tags.hpp\")\n",
    "\n",
    "# deal with repetitions\n",
    "\n",
    "job_stems = list(set(map(lambda x: \"_\".join(x.split(\"_\")[:-1]), job_dirs)))\n",
    "job_stems.sort()\n",
    "\n",
    "\n",
    "# initialize pandas (one pandas for all repetitions)\n",
    "# as all repetitions share the same columns it is easier not to initialize column names\n",
    "    \n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for job in job_stems:\n",
    "\n",
    "    \n",
    "    for repetition in range(json_repetitions):\n",
    "        \n",
    "        job_dir = job + \"_\" + str(repetition)\n",
    "        \n",
    "        logfile_path = os.path.join(job_dir, logfile_name)\n",
    "        \n",
    "        # use key value pairs to store the current values of the\n",
    "        # variable parameters\n",
    "        curr_variable_params = dict()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            logfile = open(logfile_path, mode=\"r\")\n",
    "        except OSError:\n",
    "            print(\"Filed to open {}.\".format(logfile_path))\n",
    "            exit(1)\n",
    "            \n",
    "        \n",
    "        for param_name in variable_params_names:\n",
    "        \n",
    "            param_found = False\n",
    "\n",
    "            for line in logfile:\n",
    "                for word in line.split():\n",
    "\n",
    "                    if(param_found):\n",
    "                        curr_variable_params[param_name] = int(word) # store variable parameter value               \n",
    "                        break\n",
    "\n",
    "                    if(param_name == word):\n",
    "                        param_found = True\n",
    "\n",
    "                if(param_found):\n",
    "                    break\n",
    "\n",
    "            # reset the cursor to the beginning of the file\n",
    "            logfile.seek(0)\n",
    "        \n",
    "\n",
    "        # aggregate rank data\n",
    "        \n",
    "        job_dir_contents = os.listdir(os.path.join(job_dir))\n",
    "        job_dir_contents = list(filter(lambda x: \".bin\" in x, job_dir_contents))\n",
    "\n",
    "        \n",
    "        for rank_data in job_dir_contents:  # corresponds to iteration over ranks\n",
    "            \n",
    "            rank = int(rank_data.split(\"_\")[-2])\n",
    "            log = Log(os.path.join(job_dir, rank_data), tags)\n",
    "            epochs = Epochs(log, tags)\n",
    "            \n",
    "            \n",
    "            df_curr_rank = pd.DataFrame(epochs.get_fitness_vs_time_dataframe(), \\\n",
    "                                        columns=[\"fitness\", \"wall clock time\", \"epoch\"])\n",
    "            \n",
    "            df_curr_rank[\"rank\"] = rank # varies always because of aggregation\n",
    "            df_curr_rank[\"rep\"] = repetition # varies always because of confidence interval\n",
    "            \n",
    "            for key, value in curr_variable_params.items(): # variable parameters of grid search\n",
    "                df_curr_rank[key] = value\n",
    "                \n",
    "                \n",
    "            # append to pandas (one pandas for all repetitions and ranks)\n",
    "            \n",
    "            df = df.append(df_curr_rank, ignore_index=True)\n",
    "            \n",
    "            # end loop over ranks\n",
    "\n",
    "        \n",
    "        # end loop over repetitions\n",
    "\n",
    "    \n",
    "    # plot pandas\n",
    "    \n",
    "    # end loop over job stems\n",
    "\n",
    "\n",
    "# combine all parameters of the grid search into a single column which\n",
    "# can be used as a hue for seaborn\n",
    "\n",
    "hue_grid_search = \" \".join(variable_params_names);\n",
    "\n",
    "df[hue_grid_search] = list(zip(*[df[variable_param] for \\\n",
    "                                                 variable_param in variable_params_names]))\n",
    "\n",
    "for variable_param in variable_params_names:\n",
    "    df = df.drop(variable_param, 1) # 1 indicates column\n",
    "\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"grid_searchIch .gz\", compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take out rank variation\n",
    "new_df = df.groupby([\"epoch\", \"rep\", hue_grid_search], as_index=False).agg({\"fitness\" : \"min\", \"wall clock time\" : \"max\"})\n",
    "new_df = new_df.drop(columns=\"wall clock time\")\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = new_df.copy()\n",
    "\n",
    "per = 200 # 1, 5, 10, 15, 20, 50, 100, 200\n",
    "\n",
    "df_plot = df_plot.loc[df_plot[hue_grid_search].isin([(per, 1), (per, 5), (per, 25), (per, 50), \\\n",
    "                                                    (per, 75), (per, 100), (per, 250)])]\n",
    "\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.set_xlim(100, 1000)\n",
    "ax.set_ylim(7500, 11500)\n",
    "sns.lineplot(ax=ax, x=\"epoch\", y=\"fitness\", hue=hue_grid_search, legend='full', data=df_plot)\n",
    "\n",
    "fig.savefig(\"grid_search_200.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# This is for the Island Grid Search\n",
    "\n",
    "\n",
    "\n",
    "# truncation, truncation\n",
    "# truncation, dejong\n",
    "# universal, truncation\n",
    "# universal, dejong\n",
    "# tournament, truncation\n",
    "# tournament, dejong\n",
    "\n",
    "selection_string = \"tournament\" # truncation, universal, tournament\n",
    "replacement_string = \"dejong\" # truncation, dejong\n",
    "\n",
    "\n",
    "name_idx_X = 6 # migration period\n",
    "name_idx_Y = 7 # migration amount\n",
    "\n",
    "threshold_pc = 110 # set fitness threshold to 110% of best known\n",
    "best_known_solution = 7542\n",
    "\n",
    "\n",
    "\n",
    "threshold = (float(threshold_pc) / float(100)) * float(best_known_solution)\n",
    "\n",
    "\n",
    "# throw out JSON and DS_Store\n",
    "tmp_dirs = [name for name in os.listdir(data_dir) if name[-1:] == \"0\"] \n",
    "\n",
    "experiment_dirs = list()\n",
    "\n",
    "for tmp_dir in tmp_dirs:\n",
    "    \n",
    "    tmp_dir_name_list = tmp_dir.split(\"_\")\n",
    "    \n",
    "    # test if selection_string and replacement_string are\n",
    "    # both contained within tmp_dir_name_list\n",
    "    \n",
    "    if selection_string == replacement_string:\n",
    "        if tmp_dir_name_list.count(replacement_string) == 2:\n",
    "            experiment_dirs.append(tmp_dir)\n",
    "        \n",
    "    elif all(x in tmp_dir_name_list for x in [selection_string, replacement_string]):\n",
    "        experiment_dirs.append(tmp_dir)\n",
    "\n",
    "\n",
    "num_experiments = len(experiment_dirs)\n",
    "print(num_experiments)\n",
    "assert(num_experiments == 6*8)\n",
    "\n",
    "print(experiment_dirs)\n",
    "\n",
    "# filter according to selection and replacement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a map for indexing the axes\n",
    "valuesX = set()\n",
    "valuesY = set() # use sets to guarantee unique values\n",
    "\n",
    "for exp_idx, exp_dir in enumerate(experiment_dirs):\n",
    "    \n",
    "    # Extract the parameter values from the name of the experiment directory\n",
    "    \n",
    "    exp_dir_name_list = exp_dir.split(\"_\")    \n",
    "    \n",
    "    valX = int(exp_dir_name_list[name_idx_X])\n",
    "    valY = int(exp_dir_name_list[name_idx_Y])\n",
    "    \n",
    "    valuesX.add(valX)\n",
    "    valuesY.add(valY)\n",
    "    \n",
    "\n",
    "valuesX = list(valuesX) # convert to list and sort\n",
    "valuesX.sort()\n",
    "valuesY = list(valuesY)\n",
    "valuesY.sort()\n",
    "\n",
    "# create a value to index map\n",
    "value_to_idx_map_x = { val_x : i for i, val_x in enumerate(valuesX) }\n",
    "value_to_idx_map_y = { val_y : i for i, val_y in enumerate(valuesY) }\n",
    "    \n",
    "#print(value_to_idx_map_X)\n",
    "#print(value_to_idx_map_Y)\n",
    "\n",
    "\n",
    "data_heat_map = np.zeros((len(valuesX), len(valuesY))) # num_rows, num_cols\n",
    "#print(data_heat_map)\n",
    "#print(len(valuesX))\n",
    "    \n",
    "\n",
    "for exp_idx, exp_dir in enumerate(experiment_dirs):\n",
    "    \n",
    "    # determine idx_x, idx_y based on val_x, val_y\n",
    "    exp_dir_name_list = exp_dir.split(\"_\")   \n",
    "    \n",
    "    val_x = int(exp_dir_name_list[name_idx_X])\n",
    "    val_y = int(exp_dir_name_list[name_idx_Y])\n",
    "    \n",
    "    idx_x = value_to_idx_map_x[val_x]\n",
    "    idx_y = value_to_idx_map_y[val_y]\n",
    "    \n",
    "    \n",
    "    # aggregate the CSV data\n",
    "    # determine the index of the iteration where the threshold was reached\n",
    "    \n",
    "    \n",
    "    path_curr_experiment = os.path.join(data_dir, exp_dir)\n",
    "        \n",
    "    \n",
    "    # get CSV files (one file per rank)\n",
    "    rank_CSVs = [name for name in os.listdir(path_curr_experiment) if name[-3:] == \"csv\"]\n",
    "\n",
    "    path_curr_rank_CSV = os.path.join(path_curr_experiment, rank_CSVs[0])\n",
    "    # Aggregate rank data to get the best global individual for all iterations\n",
    "    df_aggregated = pd.read_csv(path_curr_rank_CSV, names = [\"idx\", \"fitness\"])\n",
    "    df_aggregated = df_aggregated.drop_duplicates(subset=\"idx\", keep=\"first\")    \n",
    "        \n",
    "    for CSV_idx in range(1, len(rank_CSVs)):\n",
    "        \n",
    "        path_curr_rank_CSV = os.path.join(path_curr_experiment, rank_CSVs[CSV_idx])\n",
    "        \n",
    "        df_tmp = pd.read_csv(path_curr_rank_CSV, names = [\"idx\", \"fitness\"])\n",
    "        df_tmp = df_tmp.drop_duplicates(subset=\"idx\", keep=\"first\")\n",
    "\n",
    "            \n",
    "        df_aggregated = df_aggregated.combine(df_tmp, np.minimum)\n",
    "    \n",
    "    \n",
    "    df_aggregated = df_aggregated.loc[df_aggregated[\"fitness\"] > threshold]\n",
    "    threshold_idx = len(df_aggregated.index)-1 # threshold index\n",
    "    \n",
    "    data_heat_map[idx_x, idx_y] = threshold_idx\n",
    "\n",
    "    \n",
    "#fig, axes = plt.subplots(figsize=(10,10))\n",
    "\n",
    "valuesY_pc =  [str(math.ceil((float(val) / float(500)) * float(100))) + \"%\" for val in valuesY]\n",
    "\n",
    "\n",
    "heatmap_plot = sns.heatmap(np.transpose(data_heat_map), \n",
    "                           vmin=0, vmax=800,\n",
    "                           xticklabels=valuesX, yticklabels=valuesY_pc,\n",
    "                           cmap=sns.cm.rocket_r)\n",
    "\n",
    "plt.yticks(rotation=0) # rotate y labels\n",
    "plt.ylabel(\"migration amount (relative)\")\n",
    "plt.xlabel(\"migration period\")\n",
    "plt.show()\n",
    "\n",
    "fig = heatmap_plot.get_figure()\n",
    "fig.savefig(\"tournament_dejong_fc.eps\")\n",
    "\n",
    "# x is period\n",
    "# y is size\n",
    "\n",
    "\n",
    "#tp = np.zeros((len(valuesX), len(valuesY)))\n",
    "\n",
    "#for i, val_x in enumerate(valuesX):\n",
    "#    for j, val_y in enumerate(valuesY):\n",
    "        \n",
    "#        tp[i,j] = math.ceil((float(val_y) / float(val_x)) / float(500) * float(100))\n",
    "        \n",
    "        \n",
    "#sns.heatmap(np.transpose(tp), \n",
    "#            xticklabels=valuesX, yticklabels=valuesY_pc,\n",
    "#            vmin=0, vmax=20,\n",
    "#            cmap=sns.cm.rocket_r)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def param_to_list(param):\n",
    "#    if param[\"type\"] == \"range\":\n",
    "#        start = param[\"min\"]\n",
    "#        end = param[\"max\"]\n",
    "#        step = 1 if \"stride\" not in param else param[\"stride\"]\n",
    "#        return list(range(start, end, step))\n",
    "#    elif param[\"type\"] == \"list\":\n",
    "#        return param[\"list\"]\n",
    "    \n",
    "\n",
    "#def create_folder_name():\n",
    "#    return 0\n",
    "\n",
    "\n",
    "#parameters = [\"selection_policy\", \"topology\", \"replacement_policy\", \n",
    "#              \"migration_period\", \"migration_amount\", \"rank\"]\n",
    "\n",
    "#grid_parameters = [\"migration_period\", \"migration_amount\"]\n",
    "\n",
    "#idxX = parameters.index(grid_parameters[0])\n",
    "#idxY = parameters.index(grid_parameters[1])\n",
    "\n",
    "#print(idxX)\n",
    "#print(idxY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read JSON and parse parameters -> use this to identify files\n",
    "\n",
    "#all_names = os.listdir(directory_name)\n",
    "\n",
    "# Validate JSON\n",
    "#json_filename = list(filter(lambda x: \".json\" in x, all_names))\n",
    "\n",
    "#if len(json_filename) == 0:\n",
    "#    print(\"No JSON file specifying the experiment was not fond in directory {}. \".format(directory_name))\n",
    "#    exit(1)\n",
    "#if len(json_filename) > 1:\n",
    "#    print(\"Found multiple JSON files ({}) in directory {}.\".format(json_filename, directory_name))\n",
    "\n",
    "#json_filename = json_filename[0]\n",
    "\n",
    "\n",
    "#try:\n",
    "#    json_file = open(os.path.join(directory_name, json_filename), mode=\"r\")    \n",
    "#except OSError:\n",
    "#    print(\"Failed to open file {}.\".format(json_filename))\n",
    "#    exit(1)\n",
    "    \n",
    "#experiment_spec = json.load(json_file)\n",
    "#json_file.close()\n",
    "\n",
    "\n",
    "#experiment_name = experiment_spec[\"name\"]\n",
    "\n",
    "#variable_params_names = list()\n",
    "#value_lists = list()\n",
    "\n",
    "#for param_name, param in experiment_spec[\"variable_params\"].items():\n",
    "#    param_list = param_to_list(param)\n",
    "    \n",
    "#    variable_params_names.append(param_name)\n",
    "#    value_lists.append(param_list)\n",
    "\n",
    "\n",
    "# debug\n",
    "#print(variable_params_names)\n",
    "#print(value_lists)\n",
    "\n",
    "\n",
    "#for idx, element in enumerate(itertools.product(*value_lists)):\n",
    "    \n",
    "#    nam = \"_\".join(str(v) for v in element).replace(\".\", \"\").replace(\"-\", \"\")\n",
    "#    nam += \"_\" + str(0) # NO REPETITIONS\n",
    "    \n",
    "#    print(nam)\n",
    "    \n",
    "    \n",
    "#variable_params_names.sort()\n",
    "#print(variable_params_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Provide names of exactly two parameters\n",
    "\n",
    "# Create a 2D heatmap based thereon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#smallest = float('Inf')\n",
    "#idxSmallest = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#for idx in range(0,1):\n",
    "    \n",
    "#    print(dataframes[idx])\n",
    "    \n",
    "#    print('------------------------')\n",
    "    \n",
    "#    df = dataframes[idx]\n",
    "#    print(df['fitness'].min()) # best value overall\n",
    "#    print(df['fitness'].idxmin()) # takeover index\n",
    "    \n",
    "#    if (df['fitness'].min() < smallest): # keep track of smallest value\n",
    "#        smallest = df['fitness'].min()\n",
    "#        idxSmallest = idx\n",
    "    \n",
    "#    df = df.loc[df['fitness'] > threshold]\n",
    "    \n",
    "#    print(len(df.index)-1) # threshold index\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(smallest)\n",
    "#print(idxSmallest)\n",
    "    \n",
    "\n",
    "#ax = sns.lineplot(x=\"epoch\", y=\"fitness\", hue=\"run\", data=dataframes[idxSmallest])\n",
    "#ax.set_title(experiments[idxSmallest])\n",
    "#ax\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
